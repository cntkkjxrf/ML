{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from score import load_dataset_fast, score, save_preds, score_preds, SCORED_PARTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train set \n",
      "neg 7480\n",
      "pos 7520\n",
      "Loading dev set \n",
      "neg 5020\n",
      "pos 4980\n",
      "Loading test set \n",
      "unlabeled 25000\n",
      "Loading dev-b set \n",
      "pos 994\n",
      "neg 1006\n",
      "Loading test-b set \n",
      "unlabeled 8599\n",
      "Loading train_unlabeled set \n",
      "unlabeled 50000\n"
     ]
    }
   ],
   "source": [
    "part2xy = load_dataset_fast('FILIMDB', parts=SCORED_PARTS+('train_unlabeled',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids, train_texts, train_labels = part2xy['train']\n",
    "_, train_unlabeled_texts, _ = part2xy['train_unlabeled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110599"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts = list(text for _, text, _ in part2xy.values()) # это подается в pretrain!\n",
    "total_texts = sum(len(text) for text in all_texts)\n",
    "total_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'dev', 'test', 'dev-b', 'test-b', 'train_unlabeled'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "part2xy.keys() # test, test-b, train_unlabeled - нет ответов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'After watching the first 20mn of Blanche(sorry I couldn\\'t take more of it), I have now confirmed she does not. <br /><br />Basically, this \"movie\" is an insult to the real french actors participating in this farcical piece of junk. It starts from a concept successfully used in French comedies (\"Deux heures moins le quart avant Jesus Christ\", \"La Folie des Grandeurs\",...): a historical movie with anachronic tone / dialogues. This can give brilliant results if supported by brilliant actors and a \"finesse\" of direction avoiding the dreaded \"heavy comedy\" stigma.<br /><br />Unfortunately, the horsey-faced Lou Doillon ruins everything and Blanche, instead of a comedy, just turns into an horror movie. Horror to cinephiles who want to be puzzled and shocked watching fine actors such as Decaune, Zem or Rochefort struggling in the middle of this gaudy burlesque kitchy-prissy farce.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Предобработка и токенизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_texts_better(texts):\n",
    "    for i in range(0, len(texts)):\n",
    "        texts[i] = texts[i].strip().lower()\n",
    "       # texts[i] = ' '.join(texts[i])                    # строчка для отладки функции\n",
    "        texts[i] = ' '.join(texts[i].split('<br />'))\n",
    "        texts[i] = ' , '.join(texts[i].split(','))\n",
    "        texts[i] = ' . '.join(texts[i].split('.'))\n",
    "        texts[i] = ' \" '.join(texts[i].split('\"'))\n",
    "        texts[i] = ' : '.join(texts[i].split(':'))\n",
    "        texts[i] = ' ; '.join(texts[i].split(';'))\n",
    "        texts[i] = ' ! '.join(texts[i].split('!'))\n",
    "        texts[i] = ' ( '.join(texts[i].split('('))\n",
    "        texts[i] = ' ) '.join(texts[i].split(')'))\n",
    "        texts[i] = ' / '.join(texts[i].split('/'))\n",
    "        texts[i] = ' - '.join(texts[i].split('-'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3.9 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'if the myth regarding broken mirrors would be accurate ,  everybody involved in this production would now face approximately 170 years of bad luck ,  because there are a lot of mirrors falling to little pieces here .  if only the script was as shattering as the glass ,  then  \" the broken \"  would have been a brilliant film .  now it\\'s sadly just an overlong ,  derivative and dull movie with only just a handful of remarkable ideas and memorable sequences .  sean ellis made a very stylish and elegantly photographed movie ,  but the story is lackluster and the total absence of logic and explanation is really frustrating .  i got into a discussion with a friend regarding the basic concept and  \" meaning \"  of the film .  he thinks ellis found inspiration in an old legend claiming that spotting your doppelganger is a foreboding of how you\\'re going to die .  interesting theory ,  but i\\'m not familiar with this legend and couldn\\'t find anything on the internet about this ,  neither .  personally ,  i just think  \" the broken \"  is yet another umpteenth variation on the theme of  \" invasion of the body snatchers \"  but without the alien interference .   \" the broken \"  centers on the american mcvey family living in london ,  and particularly gina .  when a mirror spontaneously breaks during a birthday celebration ,  this triggers a whole series of mysterious and seemingly supernatural events .  gina spots herself driving by in a car and follows her mirror image to an apartment building .  whilst driving home in a state of mental confusion ,  she causes a terrible car accident and ends up in the hospital .  when dismissed ,  gina feels like her whole surrounding is changing .  she doesn\\'t recognize her own boyfriend anymore and uncanny fragments of the accident keep flashing before her eyes .  does she suffer from mental traumas invoked by the accident or is there really a supernatural conspiracy happening all around her? writer / director sean ellis definitely invokes feelings of curiosity and suspense in his script ,  but unfortunately he fails to properly elaborate them .   \" the broken \"  is a truly atmospheric and stylish effort ,  but only after just half an hour of film ,  you come to the painful conclusion it shall just remain a beautiful but empty package .  there\\'s a frustratingly high amount of  \" fake \"  suspense in this film .  this means building up tension ,  through ominous music and eerie camera angels ,  when absolutely nothing has even happened so far .  by the time the actually mysteriousness kicks in ,  these tricks don\\'t have any scary effect on you anymore .  some of my fellow reviewers around here compare the film and particularly sean ellis\\' style with the repertoires of david lynch ,  stanley kubrick and even alfred hitchcock ,  but that is way ,  way \\x85  way too much honor .  ps :  what is up with that alternate spelling ;  the one with the scandinavian  \" ø \" '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "for texts in all_texts:\n",
    "    make_texts_better(texts)\n",
    "train_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenized(texts):\n",
    "    for i in range(0, len(texts)):\n",
    "        texts[i] = texts[i].split(' ')\n",
    "        texts[i] = list(filter(None, texts[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.82 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for texts in all_texts:\n",
    "    make_tokenized(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'movie', 'is', 'a', 'terrible', 'waste', 'of', 'time', '.', 'although', 'it', 'is', 'only', 'an', 'hour', 'and', 'a', 'half', 'long', 'it', 'feels', 'somewhere', 'close', 'to', '4', '.', 'i', 'have', 'never', 'seen', 'a', 'movie', 'move', 'so', 'slowly', 'and', 'so', 'without', 'a', 'purpose', '.', 'this', 'is', 'also', 'a', '\"', 'horror', '\"', 'film', 'that', 'takes', 'place', 'a', 'lot', 'of', 'the', 'time', 'during', 'daylight', '.', 'my', 'friend', 'and', 'i', 'laughed', 'an', 'insane', 'amount', 'of', 'times', 'when', 'we', 'were', 'probably', 'supposed', 'to', 'be', 'scared', '.', 'the', 'only', 'thing', 'we', 'want', 'to', 'know', 'is', 'why', 'such', 'a', 'terrible', 'movie', 'was', 'released', 'in', 'so', 'many', 'countries', '.', 'it', 'cannot', 'be', 'that', 'high', 'in', 'demand', '.', 'the', 'supermodel', 'nicole', 'petty', 'should', 'stick', 'to', 'modeling', 'because', 'although', 'she', 'is', 'beautiful', 'she', 'lost', 'her', 'accent', 'so', 'many', 'times', 'in', 'this', 'movie', ',', 'half', 'of', 'the', 'time', 'she', 'is', 'british', 'and', 'half', 'the', 'time', 'she', 'is', 'american', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Почему важно, чтобы тестовая и обучающая выборка обрабатывались одинаково?\n",
    "Чтобы наиболее корректно определить класс, нужно чтобы данные были приведены к какому-то общему виду, и, полагаясь на этот общий вид, делается обработка.\n",
    "<br/>\n",
    "Например, мы приводим все буквы к нижнему регистру, чтобы выделить одинаковые слова, написанные в разном виде. Так как смысл у них один и тот же, то будет эффективнее иметь один общий вид для них, чтобы не терять смысла и не считать одинаковые слова за разные. Так же делается и с тестовой выборкой, аналогично обучающей выделяются нужные слова в нужном виде, чтобы классификация была как можно точнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Построить словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "196603"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ngrams = {}\n",
    "\n",
    "for texts in all_texts:\n",
    "    for text in texts:\n",
    "        # униграммы\n",
    "        for ugram in text:\n",
    "            if not (ugram in ngrams):\n",
    "                ngrams[ugram] = 1\n",
    "            else:\n",
    "                ngrams[ugram] += 1\n",
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 49.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3697763"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# биграммы\n",
    "for texts in all_texts:\n",
    "    for text in texts:\n",
    "        for j in range(1, len(text)):\n",
    "            tupl = (text[j - 1], text[j])\n",
    "            if not (tupl in ngrams):\n",
    "                ngrams[tupl] = 1\n",
    "            else:\n",
    "                ngrams[tupl] += 1\n",
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "15508781"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# триграммы\n",
    "for texts in all_texts:\n",
    "    for text in texts:\n",
    "        for j in range(2, len(text)):\n",
    "            tupl = (text[j - 2], text[j - 1], text[j])\n",
    "            if not (tupl in ngrams):\n",
    "                ngrams[tupl] = 1\n",
    "            else:\n",
    "                ngrams[tupl] += 1\n",
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Отсечка до около миллиона элементов в словаре"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1111730"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "words = list(ngrams.keys())  # удалить слишком редко встречающиеся слова\n",
    "for key in words:\n",
    "     if ngrams[key] <= 4:\n",
    "        del ngrams[key]\n",
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.32 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " '.',\n",
       " ',',\n",
       " 'and',\n",
       " 'a',\n",
       " 'of',\n",
       " 'to',\n",
       " 'is',\n",
       " 'in',\n",
       " 'it',\n",
       " 'i',\n",
       " 'this',\n",
       " 'that',\n",
       " '-',\n",
       " '\"',\n",
       " 'was',\n",
       " 'as',\n",
       " 'with',\n",
       " 'for',\n",
       " 'movie',\n",
       " 'but',\n",
       " 'film',\n",
       " ('of', 'the'),\n",
       " ')',\n",
       " '(',\n",
       " ('.', 'the'),\n",
       " 'on']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "sorted_grams = sorted(ngrams, key=lambda x: int(ngrams[x]), reverse=True)\n",
    "# хочется удалить самые частовстречающиеся токены, не несущие смысла (не намекающие на тональность отзыва)\n",
    "\n",
    "sorted_grams[:27]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for item in sorted_grams[:27]:\n",
    "    del ngrams[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1111703"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1111703"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "probs = np.empty(len(ngrams), dtype=float)\n",
    "n = 0\n",
    "for key in ngrams.keys():\n",
    "    probs[n] = ngrams[key]\n",
    "    ngrams[key] = n\n",
    "    n += 1\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.8121e+04, 3.2400e+02, 7.4900e+02, ..., 5.0000e+00, 5.0000e+00,\n",
       "       5.0000e+00])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Нормализация для получения вероятностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = probs ** 0.75\n",
    "sum = np.sum(probs)\n",
    "probs = probs / sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999997"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Замена документов на набор n-грамм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15000, 10000, 25000, 2000, 8599, 50000]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_texts = []\n",
    "for texts in all_texts:\n",
    "    num_texts.append(len(texts))\n",
    "num_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15000, 25000, 50000, 52000, 60599, 110599]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(1, len(all_texts)):\n",
    "    num_texts[i] += num_texts[i - 1]\n",
    "num_texts # номер в массиве, до которого идет каждый датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110599"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_texts = num_texts[len(all_texts) - 1]\n",
    "total_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n",
      "25000\n",
      "50000\n",
      "52000\n",
      "60599\n",
      "110599\n"
     ]
    }
   ],
   "source": [
    "one_all_texts = []\n",
    "for texts in all_texts:\n",
    "    one_all_texts.extend(texts)\n",
    "    print(len(one_all_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = np.empty(total_texts, dtype=np.ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(0, total_texts):\n",
    "    docgrams = []\n",
    "    text = one_all_texts[i]\n",
    "    for word in text:\n",
    "        if word in ngrams:\n",
    "            docgrams.append(ngrams[word])\n",
    "    for j in range(1, len(text)):\n",
    "        tupl = (text[j - 1], text[j])\n",
    "        if tupl in ngrams:\n",
    "            docgrams.append(ngrams[tupl])\n",
    "    for j in range(2, len(text)):\n",
    "        tupl = (text[j - 2], text[j - 1], text[j])\n",
    "        if tuple in ngrams:\n",
    "            docgrams.append(ngrams[tupl])\n",
    "    docs[i] = np.array(docgrams, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  263,   264,    18,   265,     6,   266,   267,   234,   268,\n",
       "         269,   237,   270,   271,   272,   232,   273,   274,   275,\n",
       "          90,    92,   276,   171,    53,    18,   277,   278,   277,\n",
       "         279,   197,   280,   281,    76,   282,   283,    11,   284,\n",
       "         271,   232,   285,   286,   271, 61931, 61932, 61933, 61934,\n",
       "       61935, 61936, 61937, 61938, 61939, 61940, 61941, 61942, 61943,\n",
       "       61944, 61945, 61946, 61947, 61948, 61949, 61950, 61951, 61952,\n",
       "       61953, 61954, 61955, 61956, 61957, 61958, 61959, 61960, 61961,\n",
       "       61962, 61963, 61964, 61965, 61937, 61966, 61967, 61968, 61969,\n",
       "       61970, 61971, 61972, 61973, 61974, 61975, 61976, 61977, 61978,\n",
       "       61979, 61980, 61981, 61982, 61983, 61984, 61985, 61986, 61987,\n",
       "       61988, 61989, 61990])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Перемешать данные "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_data(docs):\n",
    "    total_texts = docs.size\n",
    "    idxs = np.arange(total_texts)\n",
    "    np.random.shuffle(idxs)\n",
    "    \n",
    "    for i in range(0, total_texts):\n",
    "        np.random.shuffle(docs[i])\n",
    "        \n",
    "    docs_ids = np.empty(total_texts, dtype=np.ndarray)\n",
    "    sh_docs = docs[idxs] # возьмем документы в порядке, заданном индексами в idxs\n",
    "    for i in range(0, total_texts): # idxs[i] - настоящий индекс текущего документа\n",
    "        docs_ids[i] = np.ones(sh_docs[i].size, dtype=int) * idxs[i]\n",
    "\n",
    "    sh_docs = np.concatenate(sh_docs) # список индексов нграмм всех документов в одномерном массиве\n",
    "    docs_ids = np.concatenate(docs_ids) # список индексов документов\n",
    "    \n",
    "    return sh_docs, docs_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sh_docs, docs_ids = shuffle_data(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   76,   266,   281,   237, 61965, 61990,   286, 61969, 61985,\n",
       "          271, 61968, 61961, 61953,   232, 61936,   270, 61950, 61974,\n",
       "        61940, 61937, 61989,   232,    18, 61932,   234, 61939, 61938,\n",
       "        61980, 61947, 61935, 61981, 61934, 61978, 61956,   274,   265,\n",
       "           92,   197,    18, 61975, 61970, 61941,   284, 61954,   268,\n",
       "        61967, 61960, 61943, 61976,   282, 61983, 61949,   264,   269,\n",
       "        61986,   279, 61946, 61973, 61971, 61948, 61982, 61942, 61966,\n",
       "          267,   263,   278,   171,   272,   283, 61944,   277, 61933,\n",
       "          271, 61951, 61931, 61984, 61977,    53, 61964, 61962, 61979,\n",
       "            6, 61972,   276,   277, 61963, 61987, 61955, 61952,   273,\n",
       "        61937,   275, 61958,    11,   280, 61945,   271, 61988, 61959,\n",
       "           90,   285, 61957]), 38628924, 38628924)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[1], docs_ids.size, sh_docs.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8-10. Разбить данные на батчи, сэмплировать негативные примеры, генератор батчей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(grams, probs, docs_ids, neg_samples, nb = 2, batch_size = 100):\n",
    "    cur_batch = 0\n",
    "    grams_sz = grams.size\n",
    "    neg_sz = grams_sz * nb\n",
    "    \n",
    "    sh_idxs = np.arange(grams_sz)\n",
    "    np.random.shuffle(sh_idxs)\n",
    "    \n",
    "    for i in range(0, grams_sz, batch_size):\n",
    "        sz = min(batch_size, grams_sz - i)\n",
    "        idxs = sh_idxs[i : i + sz]\n",
    "        yield grams[idxs], docs_ids[idxs], np.ones(sz)   # заменить ли на 1 вместо массива единиц, и 0 соотв-но\n",
    "        for k in range(nb):\n",
    "            yield neg_samples[cur_batch : cur_batch + sz], docs_ids[idxs], np.zeros(sz) # nb раз создает негативные батчи\n",
    "            cur_batch += sz\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Класс Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    sgm = 1. / (1 + np.exp(-x))\n",
    "    n = (sgm <= 0.0).astype(int) * 0.00000001\n",
    "    sgm += n\n",
    "    n = (sgm >= 1.0).astype(int) * 0.00000001\n",
    "    sgm -= n\n",
    "    return sgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2Vec:\n",
    "    def __init__(self, vocab_size, docs_size, embed_size=500):\n",
    "        self.word_embs = np.random.uniform(-0.001, 0.001, (vocab_size, embed_size))\n",
    "        self.doc_embs = np.random.uniform(-0.001, 0.001, (docs_size, embed_size))\n",
    "    \n",
    "    def train(self, words_idxs, docs_idxs, labels, eta=0.2):\n",
    "                    \n",
    "        words_batch = self.word_embs[words_idxs]\n",
    "        docs_batch = self.doc_embs[docs_idxs]\n",
    "        \n",
    "        scal_pr = np.sum(words_batch * docs_batch, axis=1)\n",
    "        sigm = sigmoid(scal_pr)\n",
    "        loss = np.sum(- labels * np.log(sigm) - (1 - labels) * np.log(1 - sigm))\n",
    "        \n",
    "        koefs = np.diagflat(- labels + sigm)\n",
    "        grad_w = np.dot(koefs, docs_batch)\n",
    "        grad_d = np.dot(koefs, words_batch)\n",
    "        \n",
    "        words_batch -= eta * grad_w\n",
    "        docs_batch -= eta * grad_d\n",
    "        \n",
    "        self.word_embs[words_idxs] = words_batch\n",
    "        self.doc_embs[docs_idxs] = docs_batch\n",
    "        \n",
    "        return loss\n",
    "        '''\n",
    "        # код если бы обрабатывали по 1 паре (нграмма, документ)\n",
    "       # eta = 0.2\n",
    "        \n",
    "        batch_size = words_idxs.size\n",
    "        \n",
    "        losses = 0.0\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            wid = words_idxs[i]\n",
    "            did = docs_idxs[i]\n",
    "            \n",
    "            scalar = np.dot(self.word_embs[wid], self.doc_embs[did])\n",
    "            sigm = sigmoid(scalar)\n",
    "            loss = - labels[i] * np.log(sigm) - (1 - labels[i]) * np.log(1 - sigm)\n",
    "            losses += loss\n",
    "            \n",
    "            koef = - labels[i] + sigm\n",
    "            grad_w = koef * self.doc_embs[did]\n",
    "            grad_d = koef * self.word_embs[wid]\n",
    "            \n",
    "            self.word_embs[wid] -= eta * grad_w\n",
    "            self.doc_embs[did] -= eta * grad_d\n",
    "        \n",
    "        return losses\n",
    "    '''\n",
    "    def classify(self, words_idxs, docs_idxs):\n",
    "        words_batch = self.word_embs[words_idxs]\n",
    "        docs_batch = self.doc_embs[docs_idxs]\n",
    "        \n",
    "        scal_pr = np.sum(words_batch * docs_batch, axis=1)\n",
    "        sigm = sigmoid(scal_pr)\n",
    "        labels = (sigm >= 0.5).astype(int)\n",
    "             \n",
    "        return labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "doc2vec = Doc2Vec(probs.size, total_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000 [0]\n",
      "25000 [0, 1]\n",
      "27000 [0, 1, 3]\n"
     ]
    }
   ],
   "source": [
    "#train_ids, train_texts, train_labels = part2xy['dev-b']\n",
    "cnt = 0\n",
    "t_labels = []\n",
    "num_labeled = []\n",
    "for item in part2xy:\n",
    "    if part2xy[item][2] != None:\n",
    "        t_labels.extend(part2xy[item][2])\n",
    "        num_labeled.append(cnt)\n",
    "        print(len(t_labels), num_labeled)\n",
    "    cnt += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['neg', 'pos', 'neg', ..., 'neg', 'pos', 'pos'], dtype='<U3'),\n",
       " array(['neg', 'pos', 'neg', ..., 'neg', 'neg', 'neg'], dtype='<U3'))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "t_labels = np.array(t_labels)\n",
    "\n",
    "train_labels, t_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, ..., 0, 1, 1]), array([0, 1, 0, ..., 0, 0, 0]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels = (train_labels == 'pos').astype(int)\n",
    "t_labels = (t_labels == 'pos').astype(int)\n",
    "\n",
    "train_labels, t_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n",
      "eta = 0.13333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79165\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 0.8258666666666666\n",
      "dev: 0.8142\n",
      "dev-b: 0.67\n",
      "\n",
      " Score: 0.81 \n",
      "\n",
      "epoch 2\n",
      "eta = 0.08888888888888889\n",
      "train: 0.8766\n",
      "dev: 0.8601\n",
      "dev-b: 0.733\n",
      "\n",
      " Score: 0.8598518518518519 \n",
      "\n",
      "epoch 3\n",
      "eta = 0.05925925925925926\n",
      "train: 0.8972666666666667\n",
      "dev: 0.8793\n",
      "dev-b: 0.7555\n",
      "\n",
      " Score: 0.8801111111111111 \n",
      "\n",
      "epoch 4\n",
      "eta = 0.03950617283950617\n",
      "train: 0.9042666666666667\n",
      "dev: 0.8915\n",
      "dev-b: 0.7605\n",
      "\n",
      " Score: 0.8888888888888888 \n",
      "\n",
      "epoch 5\n",
      "eta = 0.02633744855967078\n",
      "train: 0.9099333333333334\n",
      "dev: 0.8928\n",
      "dev-b: 0.771\n",
      "\n",
      " Score: 0.8932962962962963 \n",
      "\n",
      "epoch 6\n",
      "eta = 0.017558299039780522\n",
      "train: 0.9109333333333334\n",
      "dev: 0.8941\n",
      "dev-b: 0.7735\n",
      "\n",
      " Score: 0.8945185185185185 \n",
      "\n",
      "epoch 7\n",
      "eta = 0.011705532693187014\n",
      "train: 0.9120666666666667\n",
      "dev: 0.8947\n",
      "dev-b: 0.774\n",
      "\n",
      " Score: 0.8954074074074074 \n",
      "\n",
      "epoch 8\n",
      "eta = 0.007803688462124676\n",
      "train: 0.9116666666666666\n",
      "dev: 0.8951\n",
      "dev-b: 0.774\n",
      "\n",
      " Score: 0.8953333333333333 \n",
      "\n",
      "epoch 9\n",
      "eta = 0.005202458974749784\n",
      "train: 0.9123333333333333\n",
      "dev: 0.8968\n",
      "dev-b: 0.7755\n",
      "\n",
      " Score: 0.8964444444444445 \n",
      "\n",
      "epoch 10\n",
      "eta = 0.0034683059831665222\n",
      "train: 0.9126\n",
      "dev: 0.8969\n",
      "dev-b: 0.775\n",
      "\n",
      " Score: 0.8965925925925926 \n",
      "\n",
      "epoch 11\n",
      "eta = 0.0023122039887776813\n",
      "train: 0.9126\n",
      "dev: 0.8976\n",
      "dev-b: 0.7765\n",
      "\n",
      " Score: 0.896962962962963 \n",
      "\n",
      "epoch 12\n",
      "eta = 0.0015414693258517876\n",
      "train: 0.9128\n",
      "dev: 0.897\n",
      "dev-b: 0.7765\n",
      "\n",
      " Score: 0.8968518518518519 \n",
      "\n",
      "epoch 13\n",
      "eta = 0.001027646217234525\n",
      "train: 0.9133333333333333\n",
      "dev: 0.8971\n",
      "dev-b: 0.777\n",
      "\n",
      " Score: 0.8972222222222223 \n",
      "\n",
      "epoch 14\n",
      "eta = 0.00068509747815635\n",
      "train: 0.9133333333333333\n",
      "dev: 0.897\n",
      "dev-b: 0.777\n",
      "\n",
      " Score: 0.8971851851851852 \n",
      "\n",
      "epoch 15\n",
      "eta = 0.00045673165210423334\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "max_epoch = 15\n",
    "eta = 0.2\n",
    "d = 1.\n",
    "for epoch in range(max_epoch):\n",
    "    print('epoch', epoch + 1)\n",
    "    nb = 2\n",
    "    eta = eta / 1.5\n",
    "    print('eta =', eta)\n",
    "    \n",
    "    sh_docs, docs_ids = shuffle_data(docs)\n",
    "    neg_samples = np.random.choice(len(probs), size=sh_docs.size*nb, p=probs)\n",
    "    \n",
    "    batch_gen = batch_generator(sh_docs, probs, docs_ids, neg_samples, nb)\n",
    "    for batch in batch_gen:\n",
    "        doc2vec.train(batch[0], batch[1], batch[2], eta)\n",
    "    \n",
    "    classifier = LogisticRegression(solver='lbfgs', random_state=0)\n",
    "    t_docs = doc2vec.doc_embs[0:num_texts[1]]\n",
    "    t_docs = np.vstack([t_docs, doc2vec.doc_embs[num_texts[2] : num_texts[3]]])\n",
    "    classifier.fit(doc2vec.doc_embs[0:num_texts[0]], train_labels)\n",
    "\n",
    "   # preds = classifier.predict(t_docs)\n",
    "    print('train:', classifier.score(t_docs[0:num_texts[0]], t_labels[0:num_texts[0]]))\n",
    "    print('dev:', classifier.score(t_docs[num_texts[0]:num_texts[1]], t_labels[num_texts[0]:num_texts[1]]))\n",
    "    print('dev-b:', classifier.score(t_docs[num_texts[1]:], t_labels[num_texts[1]:]))\n",
    "\n",
    "    print('\\n Score:', classifier.score(t_docs, t_labels), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc2vec.doc_embs) # сохраняю для исследования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('doc_embs.csv', sep=',', header=False, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(doc2vec.word_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 44min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df.to_csv('word_embs.csv', sep=',', header=False, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.8846666666666667 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\79165\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# на случай если хотим проверить на текущих эмбеддингах как обучается\n",
    "classifier = LogisticRegression(solver='lbfgs', random_state=0)\n",
    "t_docs = doc2vec.doc_embs[0:num_texts[1]]\n",
    "t_docs = np.vstack([t_docs, doc2vec.doc_embs[num_texts[2] : num_texts[3]]])\n",
    "classifier.fit(doc2vec.doc_embs[0:num_texts[0]], train_labels)\n",
    "\n",
    "# preds = classifier.predict(t_docs)\n",
    "print('score:', classifier.score(t_docs, t_labels), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fire import Fire\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying train set with 15000 examples ...\n",
      "train set classified in 0.03s\n",
      "Number of correct/incorrect predictions: 13603/15000\n",
      "\n",
      "Classifying dev set with 10000 examples ...\n",
      "dev set classified in 0.05s\n",
      "Number of correct/incorrect predictions: 8878/10000\n",
      "\n",
      "Classifying test set with 25000 examples ...\n",
      "test set classified in 0.05s\n",
      "no labels for test set\n",
      "\n",
      "Classifying dev-b set with 2000 examples ...\n",
      "dev-b set classified in 0.00s\n",
      "Number of correct/incorrect predictions: 1569/2000\n",
      "\n",
      "Classifying test-b set with 8599 examples ...\n",
      "test-b set classified in 0.02s\n",
      "no labels for test-b set\n",
      "Predictions saved to preds.tsv\n",
      "\n",
      "Checking saved predictions ...\n",
      "Loading train set \n",
      "neg 7480\n",
      "pos 7520\n",
      "Loading dev set \n",
      "neg 5020\n",
      "pos 4980\n",
      "Loading test set \n",
      "unlabeled 25000\n",
      "Loading dev-b set \n",
      "pos 994\n",
      "neg 1006\n",
      "Loading test-b set \n",
      "unlabeled 8599\n",
      "Number of correct/incorrect predictions: 13603/15000\n",
      "train set accuracy: 90.69\n",
      "Number of correct/incorrect predictions: 8878/10000\n",
      "dev set accuracy: 88.78\n",
      "no labels for test set\n",
      "Number of correct/incorrect predictions: 1569/2000\n",
      "dev-b set accuracy: 78.45\n",
      "no labels for test-b set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': 90.68666666666667, 'dev': 88.78, 'dev-b': 78.45}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# код из evaluate, запускаю только когда результат лучше существующего\n",
    "\n",
    "del part2xy[\"train_unlabeled\"] # после первого запуска комментирую эту строку\n",
    "PREDS_FNAME = 'preds.tsv'\n",
    "allpreds = []\n",
    "for part, (ids, x, y) in part2xy.items():\n",
    "    print('\\nClassifying %s set with %d examples ...' % (part, len(x)))\n",
    "    st = time()\n",
    "    texts_sz = [0, 15000, 10000, 25000, 2000, 8599]\n",
    "    idx = texts_sz.index(len(x))\n",
    "    for i in range(1, 6):\n",
    "        texts_sz[i] += texts_sz[i - 1]\n",
    "\n",
    "    preds = classifier.predict(doc2vec.doc_embs[texts_sz[idx - 1] : texts_sz[idx]])\n",
    "    preds = preds.astype(object)\n",
    "    for i in range(preds.size):\n",
    "        if preds[i] == 1:\n",
    "            preds[i] = 'pos'\n",
    "        else:\n",
    "            preds[i] = 'neg'\n",
    "        \n",
    "    print('%s set classified in %.2fs' % (part, time() - st))\n",
    "    allpreds.extend(zip(ids, preds))\n",
    "\n",
    "    if y is None:\n",
    "        print('no labels for %s set' % part)\n",
    "    else:\n",
    "        score(preds, y)\n",
    "\n",
    "save_preds(allpreds, preds_fname=PREDS_FNAME)\n",
    "print('\\nChecking saved predictions ...')\n",
    "score_preds(preds_fname=PREDS_FNAME, data_dir='FILIMDB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
